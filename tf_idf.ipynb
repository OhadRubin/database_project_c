{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 09:53:42 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.api.java.JavaPairRDD@28af7c0b"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.10\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3.10\"\n",
    "import numpy as np\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "# from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"MinHashLSH\")\n",
    "conf.set(\"spark.debug.maxToStringFields\", \"100\")\n",
    "conf.set(\"spark.local.dir\", \"/dev/shm/pyspark_dir\") #TODO: move in arguements\n",
    "conf.set(\"spark.driver.memory\", \"64g\")\n",
    "conf.set(\"spark.executor.memory\", \"64g\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "\n",
    "from splearn.rdd import ArrayRDD\n",
    "from splearn.feature_extraction.text import SparkCountVectorizer, SparkHashingVectorizer\n",
    "from splearn.feature_extraction.text import SparkTfidfTransformer\n",
    "from splearn.decomposition import SparkTruncatedSVD\n",
    "from splearn.pipeline import SparkPipeline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "num_nodes=1\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"/home/ohadr/database_project_c/test_data/sample.csv\")\n",
    "df = df.withColumn(\"__id__\", F.monotonically_increasing_id()).cache()\n",
    "records = df.select(\"__id__\", \"text\").rdd\n",
    "text_rdd = records.map(lambda x: x[1])\n",
    "id_rdd = records.map(lambda x: x[0])\n",
    "# text_rdd.take(10)\n",
    "# recon_rdd = id_rdd.zip(text_rdd)\n",
    "# recon_rdd.take(10)\n",
    "\n",
    "# X = pd.read_csv(\"/home/ohadr/database_project_c/test_data/sample.csv\").text.tolist()\n",
    "\n",
    "X_rdd = ArrayRDD(text_rdd)  # Get SparkContext from SparkSession\n",
    "\n",
    "# Use CountVectorizer as requested\n",
    "spark_vectorizer = SparkCountVectorizer()\n",
    "\n",
    "\n",
    "dist_pipeline = SparkPipeline((\n",
    "    ('vect', spark_vectorizer),\n",
    "    ('tfidf', SparkTfidfTransformer()),\n",
    "    ('pca', SparkTruncatedSVD(n_components=2))\n",
    "))\n",
    "\n",
    "pipeline = dist_pipeline.fit(X_rdd)  # SparseRDD\n",
    "result_dist = pipeline.transform(X_rdd)  # SparseRDD\n",
    "recon_rdd = id_rdd.zip(result_dist.unblock())\n",
    "\n",
    "recon_rdd.zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, array([-0.87095063,  0.14698077])),\n",
       " (1, array([-0.69496368,  0.18212635])),\n",
       " (2, array([-0.21320697, -0.02827101])),\n",
       " (3, array([-0.63891718,  0.19311204])),\n",
       " (4, array([-0.33683344, -0.60137885])),\n",
       " (5, array([-0.1777461 , -0.61820213])),\n",
       " (6, array([-0.6378481 ,  0.19451639])),\n",
       " (7, array([-0.12278684, -0.32309029])),\n",
       " (8, array([-0.52479918, -0.01041242])),\n",
       " (9, array([-0.21968482, -0.63012058]))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.10\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3.10\"\n",
    "import numpy as np\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "# from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"MinHashLSH\")\n",
    "conf.set(\"spark.debug.maxToStringFields\", \"100\")\n",
    "conf.set(\"spark.local.dir\", \"/dev/shm/pyspark_dir\") #TODO: move in arguements\n",
    "conf.set(\"spark.driver.memory\", \"64g\")\n",
    "conf.set(\"spark.executor.memory\", \"64g\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splearn.rdd import ArrayRDD\n",
    "from splearn.feature_extraction.text import SparkHashingVectorizer\n",
    "from splearn.feature_extraction.text import SparkTfidfTransformer\n",
    "from splearn.pipeline import SparkPipeline\n",
    "\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "X = pd.read_csv(\"/home/ohadr/database_project_c/test_data/sample.csv\").text.tolist()\n",
    "\n",
    "X_rdd = ArrayRDD(spark.parallelize(X, 4))  # sc is SparkContext\n",
    "\n",
    "# local_pipeline = Pipeline((\n",
    "#     ('vect', HashingVectorizer()),\n",
    "#     ('tfidf', TfidfTransformer())\n",
    "# ))\n",
    "dist_pipeline = SparkPipeline((\n",
    "    ('vect', SparkHashingVectorizer()),\n",
    "    ('tfidf', SparkTfidfTransformer())\n",
    "))\n",
    "\n",
    "# result_local = local_pipeline.fit_transform(X)\n",
    "result_dist = dist_pipeline.fit_transform(X_rdd)  # SparseRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install joblibspark\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblibspark import register_spark\n",
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "register_spark() # register spark backend\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC(gamma='auto')\n",
    "\n",
    "clf = GridSearchCV(svr, parameters, cv=5)\n",
    "\n",
    "with parallel_backend('spark', n_jobs=3):\n",
    "    clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "# lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pca  = PCA(k=5, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, pca])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "# test = spark.createDataFrame([\n",
    "#     (4, \"spark i j k\"),\n",
    "#     (5, \"l m n\"),\n",
    "#     (6, \"spark hadoop spark\"),\n",
    "#     (7, \"apache hadoop\")\n",
    "# ], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hashingTF = HashingTF()\n",
    "\n",
    "tf = hashingTF.transform(text_rdd.map(lambda line: line.split(\" \")))\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF(minDocFreq=0)\n",
    "model = idf.fit(tf)\n",
    "tfidf = model.transform(tf)\n",
    "mat = RowMatrix(tfidf)\n",
    "svd = mat.computeSVD(5, computeU=False)\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "\n",
    "print(\"Singular values are: %s\" % s)\n",
    "print(\"V factor is:\\n%s\" % V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\n",
    "        \"(%d, %s) --> prob=%s, prediction=%f\" % (\n",
    "            rid, text, str(prob), prediction   # type: ignore\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.numCols(),U.numRows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V.numRows,V.numCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "# $example off$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"PythonSVDExample\")\n",
    "\n",
    "    # $example on$\n",
    "    rows = sc.parallelize([\n",
    "        Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "        Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "        Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "    ])\n",
    "\n",
    "    mat = RowMatrix(rows)\n",
    "\n",
    "    # Compute the top 5 singular values and corresponding singular vectors.\n",
    "    svd = mat.computeSVD(5, computeU=True)\n",
    "    U = svd.U       # The U factor is a RowMatrix.\n",
    "    s = svd.s       # The singular values are stored in a local dense vector.\n",
    "    V = svd.V       # The V factor is a local dense matrix.\n",
    "    # $example off$\n",
    "    collected = U.rows.collect()\n",
    "    print(\"U factor is:\")\n",
    "    for vector in collected:\n",
    "        print(vector)\n",
    "    print(\"Singular values are: %s\" % s)\n",
    "    print(\"V factor is:\\n%s\" % V)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "\n",
    "# $example off$\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"TFIDFExample\")  # SparkContext\n",
    "\n",
    "# $example on$\n",
    "# Load documents (one per line).\n",
    "documents = sc.textFile(\"data/mllib/kmeans_data.txt\").map(lambda line: line.split(\" \"))\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)\n",
    "# $example off$\n",
    "\n",
    "print(\"tfidf:\")\n",
    "for each in tfidf.collect():\n",
    "    print(each)\n",
    "\n",
    "print(\"tfidfIgnore:\")\n",
    "for each in tfidfIgnore.collect():\n",
    "    print(each)\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
